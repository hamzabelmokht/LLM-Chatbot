# LLM Chatbot (Local GPU)

This project is a **local LLM chatbot** that runs on my machine using **Ollama** for GPU-accelerated inference.  
All model execution happens locally â€” no cloud LLM APIs are used.

## What it does so far
- Runs a local LLM through Ollama
- Supports basic conversational input/output
- Uses environment variables for configuration
- Currently tested on an **NVIDIA RTX 2060 Ti**

## Planned
- Add **Retrieval-Augmented Generation (RAG)** so the chatbot can answer questions using external or local data sources

## Requirements
- Ollama installed locally
- Python
- GPU recommended (currently using RTX 2060 Ti)

## Status
In progress
